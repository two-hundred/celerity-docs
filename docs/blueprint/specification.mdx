---
sidebar_position: 2
---

import JSONSchemaViewer from "@theme/JSONSchemaViewer";
import Schema from "@site/static/schemas/blueprint-spec-v2021-10-20.json";

# Specification

**v2023-04-20**

This section provides the first version of the Blueprint Specification with accompanying examples.

This is a specification of resources that can be deployed such as a Celerity application.

This specification is agnostic to the type of resources and is similar to tools like Terraform in the fact that it is abstract enough to support all kinds of resources (e.g. Cloud service resources, REST API resources, Docker containers etc.)

A blueprint will typically contain a bundle of resources that represent a logical deployable unit. One example of this would be a micro-service and its associated data stores.

The blueprint spec utilises a way of defining labels and link selectors that can be used to automatically link resources together without in-depth knowledge
of what powers the relationships. 
For example, this could abstract away a significant portion of dealing with permissions and networking for resources in a cloud provider.

The purpose of the blueprint specification and supporting implementations is to provide the backbone of scalable
code and infrastructure deployment tools and to be a part of an ecosystem of composable tools that super charge developers to deliver high quality products and services at a fast pace.

A blueprint can either be in JSON or YAML format.

## Yet another specification for resources?

You might be thinking, why on earth do we need another language/specification for defining resources?

That is a completely reasonable question. However, with a clean slate and an ability to focus on simplicity from the beginning we believe the blueprint framework having its own specification is fundamental.

What sets the blueprint specification apart from pre-existing specs of a similar nature is that it is designed to simplify relationships between resources
with pre-defined rules about how resources can be linked together that are implemented as a part of providers in an implementation of the spec such as the blueprint framework.

This means as a user, you will only have to define relationships between resources with labels, link selectors and accompanying annotations and not have to deal with the inner workings
of said relationships. This is a huge advantage especially when dealing with resources with very complex relationships.

## Spec

This is a specification conveying a valid structure for a blueprint along with additional context that goes beyond the schema.

You might be familiar with some of the concepts as the specification is inspired by Terraform, AWS CloudFormation and Kubernetes.

<br/>

### version (required)

The version of the blueprint specification to use.

**type** 

string

**allowed values** 

2023-04-20

For the v2023-04-20 version of the specification, the only valid value is `2023-04-20`.

<br/>

### transform

One or more specialised transforms to be applied to the template at the preprocessing stage.

An example use case for a transform would be to provide "virtual" resources that abstract away a lot of complexity
that can then be expanded into the underlying resources as a part of preprocessing.

**type** 

string | array[string]

**example** 

celerity-2023-04-20

*Representing a hypothetical transform for a celerity application.*

<br/>

### variables

Variables provide a way to add dynamic values that can be referenced in resources, data sources in the spec along with the include section
that allows importing child blueprints.

Variables can be referenced in resources and data sources using the following syntax:

```
${variables.[variableName]}
```

For example:
```
${variables.databaseHost}
```

**type**

mapping[name(string), [variableDefinition](#variabledefinition)]

**example**

JSON
```json
{
  "variables": {
    "databaseHost": {
      "type": "string",
      "description": "The host of the database"
    },
    "databasePort": {
      "type": "integer",
      "description": "The port of the database"
    },
    "databaseUser": {
      "type": "string",
      "description": "The user of the database to connect with"
    },
    "databasePassword": {
      "type": "string",
      "description": "The password of the user to connect to the database with",
      "secret": true
    },
    "instanceSize": {
      "type": "aws/ec2/instanceSize",
      "description": "The size of the instance to use for the service",
      "default": "t3.micro"
    },
    "deploymentTarget": {
      "type": "string",
      "description": "Whether the application should be deployed as a containerised service or cloud functions",
      "allowedValues": ["container", "cloudFunctions"],
      "default": "container"
    }
  }
}
```

YAML
```yaml
variables:
  databaseHost:
    type: string
    description: The host of the database
  databasePort:
    type: integer
    description: The port of the database
  databaseUser:
    type: string
    description: The user of the database to connect with
  databasePassword:
    type: string
    description: The password of the user to connect to the database with
    secret: true
  instanceSize:
    type: aws/ec2/instanceSize
    description: The size of the instance to use for the service
    default: t3.micro
  deploymentTarget:
    type: string
    description: Whether the application should be deployed as a containerised service or cloud functions
    allowedValues:
      - container
      - cloudFunctions
    default: container
```
<br/>

### dataSources

Data sources provide a way to source dynamic values to be used in resources and other data sources from external sources that will have been created outside
the scope of a blueprint.

Data sources can be from any provider configured in an implementation of the spec.

The order in which data sources are resolved is implicitly determined by references between data sources.

Data sources can be referenced in resources using the following syntax:
```
${dataSources.[dataSourceName].[exportedField]}
```

For example, for a data source definition like the following:

```yaml
dataSources:
  network:
    type: aws/vpc
    metadata:
      displayName: Network source
    filter:
      field: tags
      operator: has key
      search: ${variables.environment}
    exports:
      subnets:
        type: array
      securityGroups:
        type: array
      vpc:
        type: string
        aliasFor: vpcId
```

You will be able to access the id of the VPC that was found using the filter like so:

```
${dataSources.network.vpc}
```

**type**

mapping[string, [dataSourceDefinition](#datasourcedefinition)]

**example**

JSON

```json
{
  "dataSources": {
    "network": {
      "type": "aws/vpc",
      "metadata": {
        "displayName": "Network source"
      },
      "filter": {
        "field": "tags",
        "operator": "has key",
        "search": "${variables.environment}"
      },
      "exports": {
        "subnets": {
          "type": "array"
        },
        "securityGroups": {
          "type": "array"
        },
        "vpc": {
          "type": "string",
          "aliasFor": "vpcId"
        }
      }
    }
  }
}
```

YAML

```yaml
dataSources:
  network:
    type: aws/vpc
    metadata:
      displayName: Network source
    filter:
      field: tags
      operator: has key
      search: ${variables.environment}
    exports:
      subnets:
        type: array
      securityGroups:
        type: array
      vpc:
        type: string
        aliasFor: vpcId
```

<br/>

### resources (required)

Resources are the most important part of the blueprint spec, providing a way to define the key components that make up a blueprint
and the relationships between them.

An implementation of the spec is responsible for managing the lifecycle of resources, synchronising with underlying services
interfaced with through providers and keeping track of their state.

Resources in the blueprint spec are not meant to be tied to any specific notion of a resource.
If there is a resource/entity as a part of a domain model in any type of system and you can build
a provider for it, then it can be represented as a resource in a blueprint.

The most common use cases are likely to be managing cloud infrastructure and backend applications. (e.g. AWS services or a Celerity application)

When defined in a blueprint, resources are mappings keyed by a name for the resource that is unique to a blueprint.

**type**

mapping[string, [resourceDefinition](#resourcedefinition)]

**example**

JSON

```json
{
  "resources": {
    "saveOrderFunction": {
      "type": "aws/lambda/function",
      "description": "The function responsible for saving a new order to the system.",
      "metadata": {
        "displayName": "Save Order Function",
        "annotations": {
          "aws.lambda.function.populateEnvVars": true
        }
      },
      "linkSelector": {
        "byLabel": {
          "service": "ordersApi"
        }
      },
      "spec": {
        "functionName": "ordersApi-${variables.environment}-saveOrderFunction-v1",
        "codeUri": "./orders",
        "handler": "save_order.handler",
        "runtime": "python3.9",
        "tracing": "Active",
        "architectures": "arm64",
        "environment": {
          "variables": {
            "DATABASE_HOST": "${variables.databaseHost}",
            "DATABASE_PORT": "${variables.databasePort}",
            "DATABASE_USER": "${variables.databaseUser}",
            "DATABASE_PASSWORD": "${variables.databasePassword}",
            "DATABASE_NAME": "${variables.databaseName}"
          }
        },
        "timeout": 120
      }
    }
  }
}
```

YAML
  
```yaml
resources:
  saveOrderFunction:
    type: aws/lambda/function
    description: The function responsible for saving a new order to the system.
    metadata:
      displayName: "Save Order Function"
      annotations:
        aws.lambda.function.populateEnvVars: true
    linkSelector:
      byLabel:
        service: 'ordersApi'
    spec:
      functionName: ordersApi-${variables.environment}-saveOrderFunction-v1
      codeUri: ./orders
      handler: save_order.handler
      runtime: python3.9
      tracing: Active
      architectures: arm64
      environment:
        variables:
          DATABASE_HOST: ${variables.databaseHost}
          DATABASE_PORT: ${variables.databasePort}
          DATABASE_USER: ${variables.databaseUser}
          DATABASE_PASSWORD: ${variables.databasePassword}
          DATABASE_NAME: ${variables.databaseName}
      timeout: 120
```

<br/>

### include

Include provides a way to include other blueprints in a given blueprint.
Included blueprints are treated as children and their properties can be accessed from the parent using `${children.{childName}.{property}}`.
Child blueprints can be referenced in resources, as inputs to other child blueprints and in exports.

This is the primary way to compose blueprints that is a part of the core specification, see [Modular Blueprints](#modular-blueprints) for other approaches.

The order of deployment and change staging for child blueprints is determined based on references to the outputs of one child blueprint used as
the input to another.

**type**

mapping[string, [includeDefinition](#includedefinition)]

**example**

JSON

```json
{
  "include": {
    "coreInfra": {
      "path": "core-infra.yaml",
      "description": "core infra (including database) for the Orders API",
      "variables": {
        "databaseName": "${variables.databaseName}"
      },
      "metadata": {
        "sourceType": "aws/s3",
        "bucket": "order-system-blueprints",
        "region": "eu-west-1"
      }
    },
    "ordersApi": {
      "path": "api.yaml",
      "description": "The stack for the Orders API",
      "variables": {
        "databaseHost": "${children.coreInfra.databaseHost}",
        "databasePort": "${children.coreInfra.databasePort}",
        "databaseUser": "${children.coreInfra.databaseUser}",
        "databasePassword": "${children.coreInfra.databasePassword}",
        "databaseName": "${variables.databaseName}"
      }
    }
  }
}
```

YAML

```yaml
include:
  coreInfra:
    path: core-infra.yaml
    description: core infra (including database) for the Orders API
    variables:
      databaseName: ${variables.databaseName}
    metadata:
      sourceType: aws/s3
      bucket: order-system-blueprints
      region: eu-west-1
  ordersApi:
    path: api.yaml
    description: The stack for the Orders API
    variables:
      databaseHost: ${children.coreInfra.databaseHost}
      databasePort: ${children.coreInfra.databasePort}
      databaseUser: ${children.coreInfra.databaseUser}
      databasePassword: ${children.coreInfra.databasePassword}
      databaseName: ${variables.databaseName}
```

<br/>

### exports

Exports are a way to define a set of fields from resources that are publicly accessible attributes of
the blueprint that can be used in other blueprints as data sources or in external applications or systems that interface with
an implementation of the spec via an API.

**type**

mapping[string, [exportDefinition](#exportdefinition)]

**example**

JSON

```json
{
  "exports": {
    "saveOrdersFunctionArn": {
      "type": "string",
      "description": "The ARN of the function used to save orders to the system.",
      "field": "resources.saveOrdersFunction.state.functionArn"
    },
    "saveOrdersFunctionName": {
      "type": "string",
      "description": "The name of the function used to save orders to the system.",
      "field": "resources.saveOrdersFunction.spec.functionName"
    }
  }
}
```

```yaml
exports:
  saveOrdersFunctionArn:
    type: string
    description: The ARN of the function used to save orders to the system.
    field: resources.saveOrdersFunction.state.functionArn
  saveOrdersFunctionName:
    type: string
    description: The name of the function used to save orders to the system.
    field: resources.saveOrdersFunction.spec.functionName
```

<br/>


### metadata

Metadata is for blueprint-level metadata used in transformers and resource providers to carry out functionality
that spans multiple resources in a blueprint.

**type**

mapping[string, ( string | object | array | boolean | float | integer ) ]

**example**

JSON

```json
{
  "metadata": {
    "function.builder": "ESM",
    "function.builder.minify": false
  }
}
```

```yaml
metadata:
  function.builder: ESM
  function.builder.minify: false
```

<br/>

## Spec Data Types

### variableDefinition

A definition for a variable that can be referenced in resources and data sources in the spec.
The name of the variable is not in this definition as it is the key in the mapping of variables that makes use of this data type.

The custom type (`{customType}`) referenced in the variable fields is a type defined by a specific provider in an implementation of the spec
that exists for convenience when dealing with variables with a large set of fixed possible values.

A custom variable type must be of the format `{provider}/{type}`.
An example of a custom variable type would be `aws/region` with a fixed set of supported AWS regions served by an AWS resource provider.

#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>type (required)</strong></p>

The type of the variable that can be referenced throughout the spec.

**field type**

string

**allowed values** 

string | integer |  float | boolean | {customType}

___

<p style={{fontSize: '1.2em'}}><strong>allowedValues</strong></p>

A list of allowed values for the variable.
All possible values must be of the same type
as the one defined for the field.

_Boolean variables do not support allowed values as binary enumeration does not make much sense,
it is better to set boolean variables that can be true or false and use other types for enumerable lists of options._

**field type** 

array[conditional based on "type" ( string | integer | float | {customType} )]

___

<p style={{fontSize: '1.2em'}}><strong>description</strong></p>

A description of the variable.

**field type**

string

___

<p style={{fontSize: '1.2em'}}><strong>secret</strong></p>

Indicates whether or not the variable is a secret. This is useful as it allows implementations to mask sensitive values in a blueprint.

**field type** 

boolean

**default value**

false

___

<p style={{fontSize: '1.2em'}}><strong>default</strong></p>

Provides a default value for the variable as a fallback when a value for the variable is not provided.

**field type**

conditional based on "type" ( string | integer | float | boolean | {customType} )


<br/>
<br/>

### dataSourceDefinition

A definition for a data source that can be referenced in resources and other data sources in the spec.
The name of the data source is not in this definition as it is the key in the mapping of data sources that makes use of this data type.

#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>type (required)</strong></p>

The type of data source, must be a valid namespace for a type of one of the providers configured with the
spec implementation/engine that is being used. (e.g. `aws/vpc`)

**field type** 

string

___

<p style={{fontSize: '1.2em'}}><strong>metadata</strong></p>

Metadata provides useful information about the data source including things like
annotations that can be used to provide extra context about the data source which can
be used by provider implementations. 

**field type** 

[dataSourceMetadataDefinition](#datasourcemetadatadefinition)

___

<p style={{fontSize: '1.2em'}}><strong>filter (required)</strong></p>

Filter provides a way to select a specific resource managed outside of the blueprint for a data source.
A filter can simply be an equality check (`=` or `!=`), or it can be a more complex search using operators like `in`, `not in` etc.

In the case there are multiple externally-managed resources that match the filter, the first one should be used.

**field type** 

[dataSourceFilterDefinition](#datasourcefilterdefinition)

___

<p style={{fontSize: '1.2em'}}><strong>exports (required)</strong></p>

Exports provides a way to define the fields that should be exported from the data source and exposed to resources
and other data sources in a blueprint.

The names of the fields exported from the data source should be the keys in the mapping of exports
unless the `aliasFor` property is used, in which case the value of `aliasFor` should be a valid field name
in the data source object.

Object paths using dot notation can be used for exported field names (mapping keys) for nested fields in external resources (e.g. `metadata.name`)

**field type** 

mapping[name(string), [dataSourceExportDefinition](#datasourceexportdefinition)]

<br/>
<br/>

### dataSourceMetadataDefinition

A definition for the metadata that can be associated with a data source.

#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>displayName</strong></p>

A name of the data source that should be used when displaying it to the user in UI or CLI tools.

**field type** 

string

___

<p style={{fontSize: '1.2em'}}><strong>annotations</strong></p>

Annotations provide extra context that can be used by data source implementations.

Annotations should be well documented by providers for data sources
to allow for easy discovery of the available options and required context needed for a data
source to do its job.

It is good practise to namespace annotation keys and use dot notation.

**field type** 

mapping[string, string]

___

<p style={{fontSize: '1.2em'}}><strong>custom</strong></p>

Custom is set aside for any custom metadata outside the scope of the blueprint specification
implementation.

An implementation of the spec should persist the custom metadata associated with a blueprint instance
so that it can be accessed by other tools that may use the custom metadata.
Providers can also make use of the custom metadata where annotations do not suffice, however its usage must be
well documented.

An example use case would be for visual information in a UI diagramming tool for resources and data sources.

**field type** 

mapping[string, ( string | object | integer | float | array | boolean )]

<br/>
<br/>

### dataSourceFilterDefinition

A definition of a filter that can be used to select a specific resource managed outside of the blueprint for a data source.

#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>field (required)</strong></p>

The name of the field in the external resource that should be compared to the search value(s) using the configured operator.
Object paths using dot notation can be used for nested fields in external resources (e.g. `metadata.name`).

**field type** 

string

___

<p style={{fontSize: '1.2em'}}><strong>operator (required)</strong></p>

The operator used to compare the search value(s) with the configured field in the external resource.

Operators should be read with the field on the left and the search value(s) on the right.

**field type** 

string

**allowed values**

= | != | in | not in | has key | not has key | contains | not contains | starts with | not starts with | 
ends with | not ends with

For more precise information on how the operators should behave with certain inputs, see the [Operator Behaviours](#operator-behaviours) section.

___

<p style={{fontSize: '1.2em'}}><strong>search (required)</strong></p>

A value or list of values that should be compared to the value of the configured field in the resource using the configured operator.

**field type** 

string | integer | float | boolean | array[ ( string | integer | float | boolean ) ]

<br/>
<br/>

### dataSourceExportDefinition

A definition of an exported field from a data source that is exposed to resources and other data sources in
a blueprint.

#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>type (required)</strong></p>

The type of the field being exported from the data source.

**field type** 

array | string | integer | float | boolean

_only arrays of primitive values are supported as exported data source fields._

___

<p style={{fontSize: '1.2em'}}><strong>aliasFor</strong></p>

Determines the name of the field in the data source object that should be exported
to the blueprint with an alias.

When this field is set, the alias would be the key in the `dataSources` mapping.
See the [dataSources](#datasources) examples for more information.

Object paths using dot notation can be used for nested fields in external resources (e.g. `metadata.name`)

**field type** 

string

___

<p style={{fontSize: '1.2em'}}><strong>description</strong></p>

A description of the exported field that can be used to provide extra context in UI or CLI tools
that make use of the blueprint specification.

**field type** 

string

<br/>
<br/>

### resourceDefinition

A definition for a resource in a blueprint.

#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>type (required)</strong></p>

The type of the resource, namespaced by its provider.

The format for a resource type should be one of the following:

`{provider}/{service}/{resourceType}` 

`{provider}/{resourceType}`
<br/>

An example of a valid namespace for a resource type would be `aws/ec2/instance` or `celerity/handler`.

**field type** 

string

___

<p style={{fontSize: '1.2em'}}><strong>metadata</strong></p>

Metadata provides useful information about the resource including things like
annotations and labels.

For resources, annotations can be used to provide optional and required context for relationships between resources (linking).
Find out more about annotations in the [annotations](#annotations) section.

For resources, labels are essential for an implementation of the specification to be
able to link resources together in an implicit, declarative fashion.
Find out more about labels in the [labels and implicit linking](#labels-and-implicit-linking) section.

**field type** 

[resourceMetadataDefinition](#resourcemetadatadefinition)

___

<p style={{fontSize: '1.2em'}}><strong>linkSelector</strong></p>

A specification of selectors that can be used to link to resources that match the given criteria where there is an implementation
in a provider that facilitates the relationship.

linkSelector should be set for a resource that is going to be making use of other resources that match the criteria.
For example, a cloud function that will make use of a cloud data store will set a linkSelector criteria to link out to
the cloud data store.

**field type** 

[linkSelectorDefinitions](#linkselectordefinitions)

___

<p style={{fontSize: '1.2em'}}><strong>spec</strong></p>

The specification for the resource that will be used to create/update and synchronise blueprint lifecylce state with the resource in the provider.

An example for a cloud function resource spec in AWS would be the spec object in the following:

```yaml
  saveOrderFunction:
    type: aws/lambda/function
    spec:
      codeUri: ./orders
      handler: save_order.handler
      runtime: python3.9
      tracing: Active
      timeout: 120
```

The above example just shows a small subset of the fields avaiable for an AWS Lambda spec.
Resource implementations should provide a fully featured spec that conforms to the API of the provider service.

**field type** 

mapping[string, ( string | object | integer | float | array | boolean ) ]

___


<p style={{fontSize: '1.2em'}}><strong>description</strong></p>

A description of the resource that can be used to provide extra context in UI or CLI tools
that make use of the blueprint specification.

**field type** 

string

<br/>
<br/>

### resourceMetadataDefinition

A definition for the metadata that can be associated with a resource that primarily enables
implicit linking between resources when combined with selectors.


#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>displayName</strong></p>

A name of the resource that should be used when displaying it to the user in UI or CLI tools.

**field type** 

string

___

<p style={{fontSize: '1.2em'}}><strong>annotations</strong></p>

Annotations provide extra context that can be used in links that represent the relationships
between resources.

_All values that do not represent a property of a relationship between resources should be a part
of the resource spec._

Annotations should be well documented by providers for resources and their links
to allow for easy discovery of the available options and required context needed for a data
source to do its job.

It is good practise to namespace annotation keys and use dot notation.

**field type** 

mapping[string, string]

___

<p style={{fontSize: '1.2em'}}><strong>labels</strong></p>

Labels provide a way of grouping resources that can be linked to by a selector based on predefined rules.

For example, let's create a scenario where you have a serverless application that you want to deploy to a cloud service like AWS.
In this scenario you have some lambda functions that need to talk to a DynamoDB table and a Secrets Manager store.
In the blueprint specification, in order to make the DynamoDB table and SecretsManager store available to the cloud functions,
you will set a shared label on the DynamoDB table and SecretsManager store and then set a selector on the cloud functions like the following.

```yaml
resources:

  ordersTable:
    type: aws/dynamodb/table
    metadata:
      labels:
        displayName: "Orders Table"
        service: "ordersApi"
    spec:
      attributeDefinitions:
        - attributeName: "order_id"
          attributeType: "N"
        - attributeName: "product_id"
          attributeType: "N"
        - attributeName: "amount"
          attributeType: "N"
      keySchema:
        - attributeName: "order_id"
          keyType: "HASH"
        - attributeName: "product_id"
          keyType: "RANGE"
      tableName: "orders"
    
  ordersSecrets:
    type: aws/secretsmanager/secret
    metadata:
      displayName: "Orders Secrets"
      labels:
        displayName: "Orders Secrets"
        service: "ordersApi"
    spec:
      secretName: "ordersApi"

  getOrdersFunction:
    type: aws/lambda/function
    metadata:
      displayName: "Get Orders Function"
      annotations:
        # A hypothetical annotation that indicates that
        # each resource that meets the criteria of the link selector
        # should be made available to the lambda function as an environment variable.
        aws.lambda.function.populateEnvVars: true
    linkSelector:
      byLabel:
        service: 'ordersApi'
    spec:
      codeUri: ./orders
      handler: get_orders.handler
      runtime: python3.9
      tracing: Active
      timeout: 120

  saveOrderFunction:
    type: aws/lambda/function
    metadata:
      displayName: "Save Order Function"
      annotations:
        aws.lambda.function.populateEnvVars: true
    linkSelector:
      byLabel:
        service: 'ordersApi'
    spec:
      codeUri: ./orders
      handler: save_order.handler
      runtime: python3.9
      tracing: Active
      timeout: 120
```

Some predefined rules about the relationships between lambda functions and the resources they need to talk (e.g. DynamoDB tables and SecretsManager stores)
will be used in the selection process to determine what links are supported.
The link implementation will then be responsible for ensuring everything needed to "activate" the link is provisioned.
In this example, this would include ensuring the correct roles and policies are attached to the lambda functions.
With a set of enhacement annotations indicating env var names to store DB and secret store info in, 
environment variables could also be pre-populated in lambda functions with DynamoDB and SecretsManager store details.

**field type** 

mapping[string, string]

___

<p style={{fontSize: '1.2em'}}><strong>custom</strong></p>

Custom is set aside for any custom metadata outside the scope of the blueprint specification
implementation.

An implementation of the spec should persist the custom metadata associated with a blueprint instance
so that it can be accessed by other tools that may use the custom metadata.
Providers can also make use of the custom metadata where annotations do not suffice, however its usage must be
well documented.

An example use case would be for visual information in a UI diagramming tool for resources and data sources.

**field type** 

mapping[string, ( string | object | integer | float | array | boolean )]

<br/>
<br/>

### linkSelectorDefinitions

A definition for supported link selectors that can be used to implicitly link resources together.

In this version of the specification, the only supported linkSelector type is `byLabel`.


#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>byLabel</strong></p>

A selector that can be used to select resources that match a set of labels.
When multiple labels are defined, implementations of the specification should require **ALL** labels to match.

**field type** 

mapping[string, string]

<br/>
<br/>

### includeDefinition

A definition for a child blueprint that should be included in the current blueprint.
A child blueprint can be on the same file system as the parent blueprint or sourced remotely
depending on the implementation of the spec.

#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>path (required)</strong></p>

The path of the child blueprint to include.

This can be a relative path to the current blueprint, relative to the current working directory in
a tool implementing the spec or the key/name of the blueprint in a remote file system
or object store.

Relative path example:

```
-- main-blueprint.yaml
-- child-blueprint.yaml
```

```yaml
# main-blueprint.yaml
include:
  child:
    path: child-blueprint.yaml
```


Working directory path example:


```
-- app_infra
---- main-blueprint.yaml
-- core_infra
---- child-blueprint.yaml
```

```yaml
# app_infra/main-blueprint.yaml
include:
  child:
    path: ${workingDir}/core_infra/child-blueprint.yaml
```

Remote file system example:

```yaml
# main-blueprint.yaml
include:
  coreInfrastructure:
    path: core-infra-2023-04-20.yaml
    metadata:
      sourceType: aws/s3
      bucket: order-system-blueprints
      region: eu-west-1
```

**field type**

string

___

<p style={{fontSize: '1.2em'}}><strong>variables</strong></p>

A mapping of variable names to values to be used in the child blueprint.

The names (mapping keys) must match the variable names defined in the child blueprint.
The value type must match the same type as the variable definition in the child blueprint.

**field type**

mapping[string, ( string | integer | float | boolean ) ]

___

<p style={{fontSize: '1.2em'}}><strong>metadata</strong></p>

Metadata that is designed for implementations to provide custom context/information for dealing with a child
blueprint.

For example, an implementation would add metadata about a remote location to source a child blueprint from.

**field type**

mapping[string, ( string | object | array | boolean | float | integer ) ]

___

<p style={{fontSize: '1.2em'}}><strong>description</strong></p>

A description of the child blueprint providing context for its usage in the parent blueprint.

**field type**

string

<br/>
<br/>

### exportDefinition

A definition for a resource field exported from the blueprint that can be referenced outside of the blueprint.
The name of the exported attribute is the key in the mapping in the [exports](#exports) section of the blueprint.

#### FIELDS
___

<p style={{fontSize: '1.2em'}}><strong>type</strong></p>

The type of the exported field.

Implementations of the specification should explicitly fail with an informative error
when the exported field type does not match the type of the field being exported.

**field type**

string

**allowed values** 

string | object | integer | float | array | boolean

___

<p style={{fontSize: '1.2em'}}><strong>field</strong></p>

The name/path of the resource field to export.
Any attribute of a resource can be exported, it is not only limited to the
deployed state of the resource.

Fields from the final state of a resources can be referenced using the `state` property
following this format:

```
resources.{resourceName}.state.{field}
```

**field type**

string

**examples** 

`resources.saveOrderFunction.state.functionArn`

`resources.saveOrderFunction.spec.functionName`

`resources.saveOrderFunction.metadata.displayName`

___

<p style={{fontSize: '1.2em'}}><strong>description</strong></p>

A description of the exported field that will help in navigating the blueprints, especially useful in UI and CLI tools built around the spec.

**field type**

string

<br/>
<br/>

## Spec Behaviours

This section provides in-depth information on how parts of the spec should behave that goes beyond the schema definition.

_It's worth noting that some contextual information about spec behaviour that can be presented concisely will be found amongst the schema definitions._

### Operator Behaviours

The following sections describe how the operators should behave with different inputs.

#### Equality Operators ( = | != )

The same rules apply to the `=` and `!=` operators, `!=` will be the negation of the behaviour for the comparisons described below.

primitive = string | integer | float | boolean

<br/>

_field(primitive) **=** search(primitive)_

When the field in the external resource is a primitive and the search value is of the same primitive type then the search value must be an exact match.

_field(array[primitive]) **=** search(array[primitive])_

When the field in the external resource is an array of primitives and the search value is an array of primitives of the same type, then each search value
must match the field value in the corresponding position in the array.

<br/>

Any other combinations of search value and field type outside of those listed above should be invalid.
In these cases the filter operation should fail and the implementation should report an informative error to the user.
<br/>
<br/>

#### In Operators ( in | not in )

The same rules apply to the `in` and `not in` operators, `not in` will be the negation of the behaviour for the comparisons described below.

primitive = string | integer | float | boolean

<br/>

_field(primitive) **in** search(array[primitive])_

When the field value is a primitive and the search value is an array of the same primitive type, the field value must have an exact match with at least one of the search values.

<br/>

Any other combinations of search value and field type outside of those listed above should be invalid.
In these cases the filter operation should fail and the implementation should report an informative error to the user.
<br/>
<br/>

#### Has Key Operators ( has key | not has key )

The same rules apply to the `has key` and `not has key` operators, `not has key` will be the negation of the behaviour for the comparisons described below.

<br/>

_field(mapping[string, any]) **has key** search(string)_

When the field value is a mapping of strings to any value and the search value is a string, the field must have at least one
key that is an exact match with the search value.

<br/>

Any other combinations of search value and field type outside of those listed above should be invalid.
In these cases the filter operation should fail and the implementation should report an informative error to the user.
<br/>
<br/>

#### Contains Operators ( contains | not contains )

The same rules apply to the `contains` and `not contains` operators, `not contains` will be the negation of the behaviour for the comparisons described below.

primitive = string | integer | float | boolean

<br/>

_field(array[primitive]) **contains** search(primitive)_

When the field value is a primitive and the search value is an array of the same primitive type, the search value must have an exact match with at least one of the field values.

_field(string) **contains** search(string)_

When the field value is a string and the search value is a string, the search value must be a substring that can be found within the field value.

_field(mapping[string, primitive]) **contains** search(primitive)_

When the field value is a mapping of strings to primitive values and the search value is of the same primitive type,
the search value must be an exact match with at least one of the mapping values.
In this use case the keys should not be taken into account and the mapping should be treated as a list of values.

<br/>

Any other combinations of search value and field type outside of those listed above should be invalid.
In these cases the filter operation should fail and the implementation should report an informative error to the user.
<br/>
<br/>

#### Starts With Operators ( starts with | not starts with )

The same rules apply to the `starts with` and `not starts with` operators, `not contains` will be the negation of the behaviour for the comparisons described below.

<br/>

_field(string) **starts with** search(string)_

When the field value is a string and the search value is a string, the field value must begin with a substring that is an exact match with the search value.

<br/>

Any other combinations of search value and field type outside of those listed above should be invalid.
In these cases the filter operation should fail and the implementation should report an informative error to the user.
<br/>
<br/>

#### Ends With Operators ( ends with | not ends with )

The same rules apply to the `ends with` and `not ends with` operators, `not ends with` will be the negation of the behaviour for the comparisons described below.

<br/>

_field(string) **ends with** search(string)_

When the field value is a string and the search value is a string, the field value must end with a substring that is an exact match with the search value.

<br/>

Any other combinations of search value and field type outside of those listed above should be invalid.
In these cases the filter operation should fail and the implementation should report an informative error to the user.
<br/>
<br/>

## Modular Blueprints

A blueprint does not neccessarily need to be a single file, it can be split into multiple files and directories.

Making blueprints modular is useful for organising blueprints into logical groups and for reusing common parts of a blueprint across multiple blueprints.

There are two ways in which you can make a blueprint modular, the first is to use the `include` property to reference another blueprint file.
The second is to use a `blueprint` resource type provided by an implementation of the spec that allows the use of a blueprint with a pre-packaged set
of resources and data sources as if it is a regular resource.

The first of the two options must be available to users in all implementations of the spec, the second is optional and exactly how it is approached is at the discretion of the implementation.

### Using Include

The following section contains a set of examples of how you can pull in one or more blueprints from other local or remote files using the `include` property.

#### In the Same Directory

```
-- main-blueprint.yaml
-- core-infra.yaml
-- app-infra.yaml
```

`main-blueprint.yaml`

```yaml
version: 2023-04-20

variables:

  orderTopicType:
    type: string
    description: The type of topic to be used for order events
    default: "standard"
    allowedValues:
      - standard
      - fifo

  appRegion:
    type: aws/region
    description: The region in which the application will be deployed
    default: "eu-west-1"

include:

  coreInfrastructure:
    path: core-infra.yaml
    variables:
      orderTopicType: ${variables.orderTopicType}

  appInfrastructure:
    path: app-infra.yaml
    variables:
      # Implementations of the spec should allow users to reference exports from
      # included blueprints, correctly determining the order in which blueprints
      # are executed to ensure that exports are available when they are referenced.
      # See the include section of the spec for more information on how this should
      # work.
      orderTopicId: ${children.coreInfrastructure.ordersTopicId}
      region: ${variables.appRegion}

exports:

  coreOrdersTopic:
    type: string
    description: The unique identifier of the orders topic for the system
    field: children.coreInfrastructure.ordersTopicId
  
  apiBaseUrl:
    type: string
    description: The base URL of the API for the system
    field: children.appInfrastructure.apiBaseUrl

```

`core-infra.yaml`
```yaml
version: 2023-04-20

variables:
    orderTopicType:
      type: string
      description: The type of topic to be used for order events
      default: "standard"
      allowedValues:
        - standard
        - fifo

resources:
  ordersTopic:
    type: aws/sns/topic
    description: The topic to which order events will be published
    spec:
      topicType: ${variables.orderTopicType}

exports:
    ordersTopicId:
      type: string
      description: The unique identifier of the orders topic for the system
      field: resources.ordersTopic.id
```

`app-infra.yaml`

```yaml
version: 2023-04-20

variables:
  region:
    type: aws/region
    description: The region in which the application will be deployed
    default: "eu-west-1"

resources:
  api:
    type: aws/api-gateway/rest-api
    description: The API for the system
    spec:
      region: ${variables.region}

exports:
  apiBaseUrl:
    type: string
    description: The base URL of the API for the system
    field: resources.api.endpoint
```

<br/>
<br/>

#### Relative to the Current Working Directory

```
-- app-infra
  -- main-blueprint.yaml
  -- api.yaml
-- core-infra
  -- topics.yaml
  -- event-bus.yaml
```

`app-infra/main-blueprint.yaml`

```yaml
version: 2023-04-20

variables:

  orderTopicType:
    type: string
    description: The type of topic to be used for order events
    default: "standard"
    allowedValues:
      - standard
      - fifo

  appRegion:
    type: aws/region
    description: The region in which the application will be deployed
    default: "eu-west-1"

include:

  topics:
    # Explicitly specifying working directory removes ambiguity around how paths are resolved.
    path: ${workingDir}/core-infra/topics.yaml
    variables:
      orderTopicType: ${variables.orderTopicType}

  eventBus:
    path: ${workingDir}/core-infra/event-bus.yaml
    variables:
      eventBusName: ${variables.eventBusName}

  appInfrastructure:
    path: ${workingDir}/app-infra/api.yaml
    variables:
      orderTopicId: ${children.topics.ordersTopicId}
      region: ${variables.appRegion}

exports:

  coreOrdersTopic:
    type: string
    description: The unique identifier of the orders topic for the system
    field: children.topics.ordersTopicId

  coreEventBus:
    type: string
    description: The unique identifier of the event bus for the system
    field: children.eventBus.eventBusId
  
  apiBaseUrl:
    type: string
    description: The base URL of the API for the system
    field: children.appInfrastructure.apiBaseUrl

```


`core-infra/topics.yaml`
```yaml
version: 2023-04-20

variables:
    orderTopicType:
      type: string
      description: The type of topic to be used for order events
      default: "standard"
      allowedValues:
        - standard
        - fifo

resources:
  ordersTopic:
    type: aws/sns/topic
    description: The topic to which order events will be published
    spec:
      topicType: ${variables.orderTopicType}

exports:
    ordersTopicId:
      type: string
      description: The unique identifier of the orders topic for the system
      field: resources.ordersTopic.arn
```

`core-infra/event-bus.yaml`
```yaml
version: 2023-04-20

variables:
  eventBus:
    type: string
    description: The name of the event bus to be used for order events
    default: "default"

resources:
  eventBus:
    type: aws/eventbridge/event-bus
    description: The event bus to which order events will be published
    spec:
      eventBusName: ${variables.eventBus}

exports:
    eventBusId:
      type: string
      description: The unique identifier of the event bus for the system
      field: resources.eventBus.arn
```

`app-infra/api.yaml`

```yaml
version: 2023-04-20

variables:
  region:
    type: aws/region
    description: The region in which the application will be deployed
    default: "eu-west-1"

resources:
  api:
    type: aws/api-gateway/rest-api
    description: The API for the system
    spec:
      region: ${variables.region}

exports:
  apiBaseUrl:
    type: string
    description: The base URL of the API for the system
    field: resources.api.endpoint
```

<br/>
<br/>

#### From a Remote Source

To pull in child blueprints from a remote source, implementations of the spec with their own CLIs, APIs or UIs
must provide one or more ways to allow users to fetch blueprints from a remote source.

An example would be allowing the use of a remote object store such as AWS S3 or Google Cloud Storage to store blueprints.

Implementations are expected to implement support for remote sources using the metadata section of an include entry.

For example, if the remote source was an AWS S3 bucket:

```yaml
include:
  coreInfrastructure:
    path: core-infra-2023-04-20.yaml
    metadata:
      sourceType: aws/s3
      bucket: order-system-blueprints
      region: eu-west-1
```

_This example is purely to show-case how remote sources could be implemented._

You can find out more about this in the [include](#include) section.

<br/>
<br/>

### With Blueprint Resources

The following is an example of how an implementation might go about supporting blueprints as resources.


```yaml
version: 2023-04-20
variables:

  region:
    type: aws/region
    description: The region in which the application will be deployed
    default: "eu-west-1"

  orderTopicType:
    type: string
    description: The type of topic to be used for order events
    default: "standard"
    allowedValues:
      - standard
      - fifo

resources:

  api:
    type: celerity/blueprint
    description: The API for the system
    spec:
      # Assuming a celerity/blueprint provider has a module system
      # for blueprint resources.
      module:
        name: company/ordersApi
        version: 2.0
      variables:
        region: ${variables.region}
        ordersTopicId: ${resources.coreInfra.ordersTopicId}

  coreInfra:
    type: celerity/blueprint
    description: The core infrastructure for an orders system
    spec:
      # Assuming a celerity/blueprint provider has a module system
      # for blueprint resources.
      module:
        name: company/ordersCoreInfra
        version: 1.0
      variables:
        orderTopicType: ${variables.orderTopicType}

exports:

  apiBaseUrl:
    type: string
    description: The base URL of the API for the system
    field: resources.api.endpoint

  ordersTopicId:
    type: string
    description: The unique identifier of the orders topic for the system
    field: resources.coreInfra.ordersTopicId
```
